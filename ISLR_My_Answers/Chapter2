1a) Flexible model should be used. We have extremely large training set, more flexible model will have less MSE up to some level.
One should be careful about overfitting.

1b) Less flexible model should be used. We have few observations, very unflexible model such as straight line and flexible model won't
have a very significant impact. Because of extremely large predictors flexible model has a high chance to overfit.

1c) Flexible model. If their correlation is non-linear, more flexible model would fit better to training set.

1d) Less flexible model. If we have high irreducable error we may force the model to fit the data and end up with overfit.

2a) n=500, p=3 Regression because we are working with CEO salary, numerical. Inference because we relate the outcome with p's.

2b) n=20, p=13 Classification because outcome is not numerical, 0 or 1 in this case. Prediction our aim is to predict if it is 0 or 1.

2c) same logic applies.

3) Check image Chapter2Q3 at images. Explanation of the curves can be found at the book.

4) You just have to find examples.

5) We can actually use flexible models and reduce overfitting by using methods such as regularizatoin and make it less flexible up to the
point that it becomes a viable solution. In general if we have small training set and large number of predictors we use less flexible
models. If we have large training set and small number of predictors we use more flexible model.

6) In parametric approach we make an assumption on the shape of f and write the function accordingly. Then we start to train, fit our model
to the training set. Problem reduced from finding f to finding the parameters.

In non-parametric approach we don't make assumptions about the functoin. Hence we can use greater variety of shapes and end up with a more
precise result.

Parametric functions are easier to estimate since we just find parameters. Non-parametric approach requires more data and harder to
estimate compared to parametric approach but gives better results.

7a) Basic calculation to find distance, square add take square root.

7b) Closes observation to the origin is 5th one. It is green, so when K=1 our point should be green.

7c) Closes three obserevations are 2, 5, 6. Two of them are red, one is green. So our point should be red.

7d) As K becomes larger, Bayes decision boundry becomes linear. In highly non-linear data we would expect to use a small K value.
